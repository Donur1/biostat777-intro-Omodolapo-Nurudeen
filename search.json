[
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "About Me",
    "section": "",
    "text": "EDUCATION\nUniversity of Texas El Paso — Master of Science in Bioinformatics\nDec 2024\nMorgan State University — Bachelor of Science in Biology, Minor in Chemistry, Summa Cum Laude\nMay 2021\nJohns Hopkins University — Ph.D. in Biomedical Engineering\nAug 2024 – Present\n\n\n\nRESEARCH EXPERIENCE\nIRTA Fellow, National Institute of Drug Abuse\nJan 2025 – Aug 2025\nGraduate Research Assistant, University of Texas El Paso\nJan 2023 – Dec 2024\nSummer Intern, National Institute of Drug Abuse\nJun 2024 – Aug 2024\nResearch Assistant, MSU Department of Biology\nOct 2021 – Dec 2022\nUndergraduate Research Assistant, MSU Department of Chemistry\nAug 2018 – May 2021\n\n\n\nAWARDS & HONORS\n\nVivien Thomas PhD Scholar, Johns Hopkins University\n\nWomen Auxiliary Scholar, UTEP (2024)\n\nFirst Place, Physical Sciences, AAAS E-Poster Competition (2021)\n\nAcademic Achievement & Excellence in Research Award, MSU (2021)\n\n4× Martin D. Jenkins Honors Scholar, MSU\n\n2× Power Live Residential Scholar, MSU\n\n\n\n\nPUBLICATION\nAngela J. Winstead, Dolapo Nurudeen, et al.\nOrganometallic Rhenium(I) Complexes of Some Non-Steroidal Anti-Inflammatory Drugs (ReNSAIDs): Synthesis, Characterizations, Photophysical Properties and DNA-Binding Studies. Nova Publishers, 2021.\n\n\n\nPRESENTATIONS\nOmodolapo Nurudeen, Katherine E. Savella, Olivia R. Drake, Megan B. Brenner, Diana Pham, Padmashri Saravanan, Yugantar Gera, Sophia J. Weber, Veronica A. Lennon, Lauren E. Komer, F. Javier Rubio, Lihua Wan, Kathy Schaefer, Andrew Lemire, Eric R. Schreiter, Vilas Menon, Bruce T. Hope, Rajtarun Madangopal\nIn vivo labeling and molecular characterization of cocaine memory-specific active neurons using CaMPARI2\n- Poster Presentation, NIH Summer Poster Day, Aug 1, 2024\n- Poster Presentation, NIDA Summer Poster Day, Jul 30, 2024\nOmodolapo Nurudeen, Ming-Ying Leung, Amanda Bataycan\nExamining Genomic Data to Understand Functional Effects of Genetic Variants in Ovarian Cancer\n- Poster Presentation, GLBIO Conference, May 14, 2024\n- Poster Presentation, UTEP/NMSU Joint Workshop, Oct 28, 2023\nDolapo Nurudeen, Angela Winstead, Santosh Mandal\nFac-tricarbonyl (Pentylcarbonato)(α-diimine) Rhenium Complexes\n- SACNAS NDiSTEM Digital Conference, Oct 15, 2021\n- MSU & Johns Hopkins PULSE Seminar, Feb 2, 2021\n- NTA Conference, Sep 25, 2020\n- MSU Undergraduate Summer Research Program, Jul 28, 2020 (Poster)\n- MSU Undergraduate Summer Research Program, Jun 29, 2020 (Oral)\n- JHU Diversity Symposium, Nov 7, 2019\n- UMBC Undergraduate Symposium, Oct 19, 2019\n- AAAS, Washington DC, Feb 16, 2019\n- JHU Diversity Symposium, Dec 4, 2018\n- UMBC Undergraduate Symposium, Oct 20, 2018"
  },
  {
    "objectID": "example_analysis.html",
    "href": "example_analysis.html",
    "title": "Sample Analysis",
    "section": "",
    "text": "The National Health and Nutrition Examination Survey (NHANES) is a nationally representative program that combines interviews and standardized examinations to characterize the health and nutritional status of the U.S. population. NHANES links survey records to subsequent mortality follow-up so it is a good database to explore how demographic, behavioral, and clinical factors can relate to survival. It is also important to understand how people 40 and older are at risk for chronic disease and thus death. This analysis will use the NHANES 2003–2004 cycle to build a predictive models mortality status among participants aged 40 and above. The intended audience are health data analysts and public health researchers to interested in understanding and make some predictions on patient risk based on available health data.\nThe goal of this analyisis is to explore how demographic and clinical factors relate to mortality among NHANES participants aged 40 and above. I will use histograms, boxplots, and scatterplots to identify potential trends and patterns in health indicators such as BMI, blood pressure, and diabetes status. I wll build Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) models to predict mortality status and evaluate which method provides better classification performance and better predictive understanding of mortality risk in older adults.\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.1     ✔ readr     2.1.5\n✔ ggplot2   4.0.0     ✔ stringr   1.6.0\n✔ lubridate 1.9.4     ✔ tibble    3.3.0\n✔ purrr     1.2.0     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\n\nload('/Users/omodolaponurudeene/Desktop/fall 2025/Statistical Programming/Homework 1_Part2/nhanes2003-2004.Rda')\n\nnumeric_value &lt;- c(\"RIDAGEYR\", \"BMXWAIST\", \"BPXDI1\", \"LBXRDW\", \"BMXBMI\", \"BPXSY1\")\n\nnhanes2003_2004 &lt;- nhanes2003_2004 %&gt;%\n  mutate(across(all_of(numeric_value), ~ as.numeric(as.character(.))))\n\nnhanes2003_2004 &lt;- nhanes2003_2004 %&gt;%\n  filter(RIDAGEYR &gt;= 40)\n\nnhas_subset &lt;- nhanes2003_2004 %&gt;%\n    select(\n        RIAGENDR,\n        HSD010,\n        DIQ010,\n        MCQ010,\n        MCQ160A,\n        BPQ060,\n        BMXBMI,\n        BMXWAIST,\n        BPXSY1,\n        BPXDI1,\n        mortstat\n    )\n\ncategorical_vars &lt;- c(\"RIAGENDR\", \"HSD010\", \"DIQ010\", \"MCQ010\", \"MCQ160A\", \"BPQ060\")\n\nnhas_subset[categorical_vars] &lt;- lapply(nhas_subset[categorical_vars], function(x) {\n    x[x == \"9\"] &lt;- NA\n    droplevels(x)\n})\nVariable Dictionary\n#Droping NAs\nnhas_subset &lt;- tidyr::drop_na(nhas_subset)\n\n#Sumamrizing the mean, median, mix and max of the dataset\nnhas_subset %&gt;%\n  summarise(across(where(is.numeric), list(\n    min = min,\n    mean = mean,\n    median = median,\n    max = max\n  ), na.rm = TRUE))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(...)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n  BMXBMI_min BMXBMI_mean BMXBMI_median BMXBMI_max BMXWAIST_min BMXWAIST_mean\n1       14.7    28.68119         27.99      57.67         61.8      100.3226\n  BMXWAIST_median BMXWAIST_max BPXSY1_min BPXSY1_mean BPXSY1_median BPXSY1_max\n1            99.6        157.1         80    133.8913           130        228\n  BPXDI1_min BPXDI1_mean BPXDI1_median BPXDI1_max mortstat_min mortstat_mean\n1          0    71.43836            72        120            0      0.169863\n  mortstat_median mortstat_max\n1               0            1\n\nstr(nhas_subset)\n\n'data.frame':   2190 obs. of  11 variables:\n $ RIAGENDR: Factor w/ 2 levels \"1\",\"2\": 1 2 1 2 1 2 2 2 1 1 ...\n $ HSD010  : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 2 3 3 4 2 3 4 2 4 2 ...\n $ DIQ010  : Factor w/ 3 levels \"1\",\"2\",\"3\": 2 2 2 2 2 2 2 2 2 2 ...\n $ MCQ010  : Factor w/ 2 levels \"1\",\"2\": 2 1 1 1 2 2 2 2 2 2 ...\n $ MCQ160A : Factor w/ 2 levels \"1\",\"2\": 2 2 1 2 2 2 2 1 1 1 ...\n $ BPQ060  : Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 2 1 1 1 ...\n $ BMXBMI  : num  31.3 25.5 28.3 22.6 29.2 ...\n $ BMXWAIST: num  118.4 91.4 102 87.4 107 ...\n $ BPXSY1  : num  124 128 154 130 126 164 138 124 124 132 ...\n $ BPXDI1  : num  88 86 54 86 88 70 66 54 68 82 ...\n $ mortstat: int  0 0 1 0 0 0 1 0 0 0 ...\n\n#Some columns have value 9 to represent missing so dropping that as well\nnhas_subset[categorical_vars] &lt;- lapply(nhas_subset[categorical_vars], function(x) {\n    x[x == \"9\"] &lt;- NA\n    droplevels(x)\n})\n\n\n\n#Explore some of the important predictors using ggplots \nnum_x &lt;- \"BMXBMI\"\nnum_y &lt;- \"BPXSY1\"\ncat_g &lt;- \"RIAGENDR\"\ncat_f &lt;- \"DIQ010\"\n\n\n# Histogram (geom_histogram)\nggplot(nhas_subset, aes_string(x = num_x)) +\n    geom_histogram(bins = 30) +\n    labs(\n        title = paste(\"Distribution of\", num_x),\n        subtitle = \"Basic histogram of a numeric variable\",\n        x = num_x, y = \"Count\",\n        caption = \"Source: NHAS\"\n    )\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n\n# Plot 2: Boxplot (geom_boxplot) \nggplot(nhas_subset, aes_string(x = cat_g, y = num_x)) +\n    geom_boxplot() +\n    labs(\n        title = paste(num_x, \"across\", cat_g),\n        subtitle = \"Group-wise spread and median\",\n        x = cat_g, y = num_x,\n        caption = \"Source: NHAS\"\n    )\n\n\n\n\n\n\n\n# Plot 3: Scatter with smoother + faceting (geom_point, geom_smooth, facet_wrap)\nggplot(nhas_subset, aes_string(x = num_x, y = num_y)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(se = FALSE, method = \"loess\") +\n    facet_wrap(as.formula(paste0(\"~\", cat_f))) +\n    labs(\n        title = paste(num_y, \"vs\", num_x),\n        subtitle = paste(\"Faceted by\", cat_f, \"with LOESS smoother\"),\n        x = num_x, y = num_y,\n        caption = \"Source: NHAS\"\n    )\n\n`geom_smooth()` using formula = 'y ~ x'\n# Analysis\nnhas &lt;- nhas_subset\n\nstr(nhas)\n\n'data.frame':   2190 obs. of  11 variables:\n $ RIAGENDR: Factor w/ 2 levels \"1\",\"2\": 1 2 1 2 1 2 2 2 1 1 ...\n $ HSD010  : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 2 3 3 4 2 3 4 2 4 2 ...\n $ DIQ010  : Factor w/ 3 levels \"1\",\"2\",\"3\": 2 2 2 2 2 2 2 2 2 2 ...\n $ MCQ010  : Factor w/ 2 levels \"1\",\"2\": 2 1 1 1 2 2 2 2 2 2 ...\n $ MCQ160A : Factor w/ 2 levels \"1\",\"2\": 2 2 1 2 2 2 2 1 1 1 ...\n $ BPQ060  : Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 2 1 1 1 ...\n $ BMXBMI  : num  31.3 25.5 28.3 22.6 29.2 ...\n $ BMXWAIST: num  118.4 91.4 102 87.4 107 ...\n $ BPXSY1  : num  124 128 154 130 126 164 138 124 124 132 ...\n $ BPXDI1  : num  88 86 54 86 88 70 66 54 68 82 ...\n $ mortstat: int  0 0 1 0 0 0 1 0 0 0 ...\n\nset.seed(123)\nn &lt;- nrow(nhas)\ntrain_idx &lt;- sample(seq_len(n), size = 0.8 * n)\ntrain.set &lt;- nhas[train_idx, ]\ntest.set &lt;- nhas[-train_idx, ]\n\n\n\n#Split into training and test set\nlibrary(rsample)\nsplit &lt;- initial_split(nhas, prop = 0.8, strata = mortstat)\ntrain.set &lt;- training(split)\ntest.set &lt;- testing(split)\n\ndim(test.set)\n\n[1] 439  11\n\ndim(train.set)\n\n[1] 1751   11\n# Run linear discriminate model\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:rsample':\n\n    calibration\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nsummary(train.set)\n\n RIAGENDR HSD010  DIQ010   MCQ010   MCQ160A  BPQ060       BMXBMI     \n 1:899    1:182   1: 261   1: 184   1: 681   1:1496   Min.   :14.70  \n 2:852    2:461   2:1455   2:1567   2:1070   2: 255   1st Qu.:24.84  \n          3:617   3:  35                              Median :28.00  \n          4:395                                       Mean   :28.67  \n          5: 96                                       3rd Qu.:31.78  \n                                                      Max.   :56.03  \n    BMXWAIST          BPXSY1          BPXDI1         mortstat     \n Min.   : 61.80   Min.   : 82.0   Min.   :  0.0   Min.   :0.0000  \n 1st Qu.: 90.85   1st Qu.:118.0   1st Qu.: 64.0   1st Qu.:0.0000  \n Median : 99.60   Median :130.0   Median : 72.0   Median :0.0000  \n Mean   :100.27   Mean   :134.1   Mean   : 71.6   Mean   :0.1696  \n 3rd Qu.:109.00   3rd Qu.:146.0   3rd Qu.: 80.0   3rd Qu.:0.0000  \n Max.   :157.10   Max.   :228.0   Max.   :120.0   Max.   :1.0000  \n\nsummary(test.set)\n\n RIAGENDR HSD010  DIQ010  MCQ010  MCQ160A BPQ060      BMXBMI     \n 1:221    1: 39   1: 82   1: 49   1:164   1:376   Min.   :16.60  \n 2:218    2:114   2:352   2:390   2:275   2: 63   1st Qu.:24.77  \n          3:163   3:  5                           Median :27.87  \n          4:101                                   Mean   :28.72  \n          5: 22                                   3rd Qu.:31.82  \n                                                  Max.   :57.67  \n    BMXWAIST         BPXSY1        BPXDI1          mortstat     \n Min.   : 67.8   Min.   : 80   Min.   :  0.00   Min.   :0.0000  \n 1st Qu.: 91.0   1st Qu.:117   1st Qu.: 64.00   1st Qu.:0.0000  \n Median : 99.9   Median :130   Median : 72.00   Median :0.0000  \n Mean   :100.5   Mean   :133   Mean   : 70.81   Mean   :0.1708  \n 3rd Qu.:109.7   3rd Qu.:146   3rd Qu.: 80.00   3rd Qu.:0.0000  \n Max.   :144.0   Max.   :220   Max.   :114.00   Max.   :1.0000  \n\ntrain.set %&gt;%\n    group_by(mortstat) %&gt;%\n    summarise(across(everything(), ~ length(unique(.)))) %&gt;%\n    print()\n\n# A tibble: 2 × 11\n  mortstat RIAGENDR HSD010 DIQ010 MCQ010 MCQ160A BPQ060 BMXBMI BMXWAIST BPXSY1\n     &lt;int&gt;    &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;   &lt;int&gt;  &lt;int&gt;  &lt;int&gt;    &lt;int&gt;  &lt;int&gt;\n1        0        2      5      3      2       2      2   1008      518     67\n2        1        2      5      3      2       2      2    279      216     57\n# ℹ 1 more variable: BPXDI1 &lt;int&gt;\n\nlda.fit &lt;- lda(mortstat ~ ., data = train.set)\nlda.pred &lt;- predict(lda.fit, newdata = test.set)\nprobs &lt;- lda.pred$posterior[, \"1\"]\n\n\n\n#use multiple cutoffs to see which gives best sensitivity vs specificity\ncutoffs &lt;- c(0.4, 0.5, 0.6)\n\nfor (c in cutoffs) {\n    cat(\"\\n--- Cutoff:\", c, \"---\\n\")\n    lda.class &lt;- ifelse(probs &gt; c, 1, 0)\n\n    cm &lt;- confusionMatrix(\n        factor(lda.class),\n        factor(test.set$mortstat),\n        positive = \"1\"\n    )\n\n    print(cm)\n}\n\n\n--- Cutoff: 0.4 ---\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 342  60\n         1  22  15\n                                          \n               Accuracy : 0.8132          \n                 95% CI : (0.7735, 0.8486)\n    No Information Rate : 0.8292          \n    P-Value [Acc &gt; NIR] : 0.8296          \n                                          \n                  Kappa : 0.1747          \n                                          \n Mcnemar's Test P-Value : 4.389e-05       \n                                          \n            Sensitivity : 0.20000         \n            Specificity : 0.93956         \n         Pos Pred Value : 0.40541         \n         Neg Pred Value : 0.85075         \n             Prevalence : 0.17084         \n         Detection Rate : 0.03417         \n   Detection Prevalence : 0.08428         \n      Balanced Accuracy : 0.56978         \n                                          \n       'Positive' Class : 1               \n                                          \n\n--- Cutoff: 0.5 ---\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 355  62\n         1   9  13\n                                          \n               Accuracy : 0.8383          \n                 95% CI : (0.8004, 0.8715)\n    No Information Rate : 0.8292          \n    P-Value [Acc &gt; NIR] : 0.3327          \n                                          \n                  Kappa : 0.2066          \n                                          \n Mcnemar's Test P-Value : 6.775e-10       \n                                          \n            Sensitivity : 0.17333         \n            Specificity : 0.97527         \n         Pos Pred Value : 0.59091         \n         Neg Pred Value : 0.85132         \n             Prevalence : 0.17084         \n         Detection Rate : 0.02961         \n   Detection Prevalence : 0.05011         \n      Balanced Accuracy : 0.57430         \n                                          \n       'Positive' Class : 1               \n                                          \n\n--- Cutoff: 0.6 ---\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 358  68\n         1   6   7\n                                          \n               Accuracy : 0.8314          \n                 95% CI : (0.7931, 0.8653)\n    No Information Rate : 0.8292          \n    P-Value [Acc &gt; NIR] : 0.4803          \n                                          \n                  Kappa : 0.1144          \n                                          \n Mcnemar's Test P-Value : 1.33e-12        \n                                          \n            Sensitivity : 0.09333         \n            Specificity : 0.98352         \n         Pos Pred Value : 0.53846         \n         Neg Pred Value : 0.84038         \n             Prevalence : 0.17084         \n         Detection Rate : 0.01595         \n   Detection Prevalence : 0.02961         \n      Balanced Accuracy : 0.53842         \n                                          \n       'Positive' Class : 1\nlda.class &lt;- ifelse(probs &gt; 0.3, 1, 0)\n\ncm_lda &lt;- confusionMatrix(\n    factor(lda.class),\n    factor(test.set$mortstat),\n    positive = \"1\"\n)\ncm_lda\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 314  51\n         1  50  24\n                                          \n               Accuracy : 0.7699          \n                 95% CI : (0.7277, 0.8085)\n    No Information Rate : 0.8292          \n    P-Value [Acc &gt; NIR] : 0.9994          \n                                          \n                  Kappa : 0.1836          \n                                          \n Mcnemar's Test P-Value : 1.0000          \n                                          \n            Sensitivity : 0.32000         \n            Specificity : 0.86264         \n         Pos Pred Value : 0.32432         \n         Neg Pred Value : 0.86027         \n             Prevalence : 0.17084         \n         Detection Rate : 0.05467         \n   Detection Prevalence : 0.16856         \n      Balanced Accuracy : 0.59132         \n                                          \n       'Positive' Class : 1               \n                                          \n\ntable(lda.class, test.set$mortstat)\n\n         \nlda.class   0   1\n        0 314  51\n        1  50  24\n\nmean(lda.class != test.set$mortstat)\n\n[1] 0.2300683\n# Fit QDA\nqda.fit &lt;- qda(mortstat ~ ., data = train.set)\nqda.pred &lt;- predict(qda.fit, newdata = test.set)\n\n# Use cutoff = 0.3\nqda.probs &lt;- qda.pred$posterior[, \"1\"]\nqda.class &lt;- ifelse(qda.probs &gt; 0.3, 1, 0)\n\n\ncm_qda &lt;- confusionMatrix(\n    factor(qda.class, levels = c(0, 1)),\n    factor(test.set$mortstat, levels = c(0, 1)),\n    positive = \"1\"\n)\ncm_qda\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 285  43\n         1  79  32\n                                          \n               Accuracy : 0.7221          \n                 95% CI : (0.6776, 0.7635)\n    No Information Rate : 0.8292          \n    P-Value [Acc &gt; NIR] : 1.000000        \n                                          \n                  Kappa : 0.1761          \n                                          \n Mcnemar's Test P-Value : 0.001531        \n                                          \n            Sensitivity : 0.42667         \n            Specificity : 0.78297         \n         Pos Pred Value : 0.28829         \n         Neg Pred Value : 0.86890         \n             Prevalence : 0.17084         \n         Detection Rate : 0.07289         \n   Detection Prevalence : 0.25285         \n      Balanced Accuracy : 0.60482         \n                                          \n       'Positive' Class : 1               \n                                          \n\nmean(qda.class != test.set$mortstat)\n\n[1] 0.2779043\nSummary\nBased on this models, the best in class model for Discriminant analysis is LDA at cutoff of 0.3. It achieves the best trade-off between sensitivity and specificity. It also has a better test error(21%) than QDA(30%). However, LDA provided the best tradeoff between sensitivity and specificity. This suggests that linear separation in feature space may best capture mortality risk patterns among older adults\n#Reference @misc{nhanes2003_2004, title = {National Health and Nutrition Examination Survey (NHANES) 2003–2004}, author = {{Centers for Disease Control and Prevention (CDC)}}, year = {2004}, url = {https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear=2003}, note = {Accessed November 2025}, }\n@manual{mass2024, title = {MASS: Modern Applied Statistics with S}, author = {Venables, W. N. and Ripley, B. D.}, year = {2024}, note = {R package version 7.3-60}, url = {https://cran.r-project.org/web/packages/MASS/index.html} }\n@article{natureRNA2024, title = {RNA-centric methods reveal the complex landscape of RNA function and regulation}, author = {Nature Reviews Methods Primers Editors}, journal = {Nature Reviews Methods Primers}, year = {2024}, volume = {4}, number = {1}, pages = {46}, url = {https://www.nature.com/articles/s43586-024-00346-y} }"
  },
  {
    "objectID": "example_analysis.html#functions-used",
    "href": "example_analysis.html#functions-used",
    "title": "Sample Analysis",
    "section": "Functions Used",
    "text": "Functions Used\ndplyr: select(), mutate(), filter(), %&gt;%, summarise()\nggplot2: geom_histogram(), geom_boxplot(), geom_point(), geom_smooth(), facet_wrap()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Omodolapo Nurudeen",
    "section": "",
    "text": "My Photo\n\n\n\nHi there! — I’m Omodolapo Nurudeen, a first-year BME PhD student at Johns Hopkins University.\nMy research lies at the intersection of computational genomics and neuroscience.\nI explore how emerging computational and statistical tools can deepen our understanding of the brain.\n\nFun Facts\n\nWhen I’m not doing schoolwork, you’ll probably find me exploring a new fantasy world through books or movies.\n\nI love crafting**, and I’m a huge tea enthusiasts, any type of tea, anytime ☕"
  }
]